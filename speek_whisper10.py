import streamlit as st
import numpy as np
from streamlit_webrtc import WebRtcMode, webrtc_streamer
from pydub import AudioSegment
import queue, pydub, tempfile,  os, time
import whisper
import torch
import torchaudio
import torchvision

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    Transcribe an audio segment using OpenAI's Whisper ASR system.
    Args:
        audio_segment (AudioSegment): The audio segment to transcribe.
        debug (bool): If True, save the audio segment for debugging purposes.
    Returns:
        str: The transcribed text.
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        audio = whisper.load_audio(tmpfile.name)
        audio = whisper.pad_or_trim(audio)
        # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
        #base:74M,small:244M,medium,large
        # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
        answer = result['text']
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
        if answer == "" :
            print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            return None 
        elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            return None 
        else:
            print(answer)
            return answer
    tmpfile.close()  
    os.remove(tmpfile.name)
    
        ###############################################################
         
def frame_energy(frame):
    samples = np.frombuffer(frame.to_ndarray().tobytes(), dtype=np.int16)
    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚µãƒ³ãƒ—ãƒ«ã®ä¸€éƒ¨ã‚’å‡ºåŠ› 
    #print("Samples:", samples[:10])
    # NaNã‚„ç„¡é™å¤§ã®å€¤ã‚’é™¤å» 
    #if not np.isfinite(samples).all(): 
        #samples = samples[np.isfinite(samples)]
    #np.isfinite() ã§ç„¡åŠ¹ãªå€¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã ã‘ã§ã¯ã€
    # ç©ºé…åˆ—ã®ã‚¨ãƒ©ãƒ¼ãŒå†ã³ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
    # np.nan_to_num ã‚’ä½¿ç”¨ã—ãŸã»ã†ãŒå®‰å…¨ã«å‡¦ç†ã§ãã¾ã™ã€‚
    # ç„¡åŠ¹ãªå€¤ã‚’å®‰å…¨ãªå€¤ã«ç½®æ›
    samples = np.nan_to_num(samples, nan=0.0, posinf=0.0, neginf=0.0)
    if len(samples) == 0: 
        return 0.0
    energy = np.sqrt(np.mean(samples**2)) 
    #print("Energy:", energy) 
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å‡ºåŠ› 
    return energy

def is_silent_frame(audio_frame, amp_threshold):
    """
    ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ã‹ã©ã†ã‹ã‚’æœ€å¤§æŒ¯å¹…ã§åˆ¤å®šã™ã‚‹é–¢æ•°ã€‚
    """
    samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)
    max_amplitude = np.max(np.abs(samples))
    return max_amplitude < amp_threshold

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold, amp_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã™ã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
        amp_threshold:ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚

    """
    for audio_frame in audio_frames:
        sound_chunk = add_frame_to_chunk(audio_frame, sound_chunk)

        energy = frame_energy(audio_frame)
        
        if energy < energy_threshold or is_silent_frame(audio_frame, amp_threshold):
            silence_frames += 1 
            #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
        else:
            silence_frames = 0 
            #ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚

    return sound_chunk, silence_frames

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
   
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            #if is_silent_chunk(sound_chunk):
                #print("ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯å†…ã®ã™ã¹ã¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã™")
            #else: 
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³")
            frame_length_ms = 20 
            num_frames = len(sound_chunk) // frame_length_ms 
            #print("å…¨ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼",num_frames)
            #print("ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼",silence_frames)

            if num_frames-silence_frames > 100:
                text = transcribe(sound_chunk)
                text_output.write(text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)
            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0

       #else:
            #print("ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒå°‘ãªã„handle_silenceãƒ«ãƒ¼ãƒãƒ³")
            #text = transcribe(sound_chunk)
            #text_output.write(text)

    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    Handle the case where the audio frame queue is empty.
    Args:
        sound_chunk (AudioSegment): The current sound chunk.
        text_output (st.empty): The Streamlit text output object.
    Returns:
        AudioSegment: The updated sound chunk.
    """
    if len(sound_chunk) > 0:
        print("handle_queue_emptyãƒ«ãƒ¼ãƒãƒ³")   
        text = transcribe(sound_chunk)
        text_output.write(text)
        sound_chunk = pydub.AudioSegment.empty()

    return sound_chunk

def app_sst(
        #audio_receiver_size,
        status_indicator,
        text_output,
        #energy_threshold, #=2000,  #2000
        #amp_threshold,
        #silence_frames_threshold,   #=100 #500  #100  #1024ã§Goodï¼ï¼
        #timeout=1, #3, 
        ):
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã€‚ 
    ã“ã®é–¢æ•°ã¯WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã‚’é–‹å§‹ã—ã€
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã€ä¸€å®šã®ã—ãã„å€¤ã‚’è¶…ãˆã‚‹ç„¡éŸ³ãŒç¶šã„ãŸå ´åˆã«éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚
    å¼•æ•°ï¼š
        status_indicator: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­ã¾ãŸã¯åœæ­¢ä¸­ï¼‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        text_output: èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        timeout (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¬ã‚·ãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ç§’ã€‚
        energy_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ã¨è¦‹ãªã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000ã€‚
        silence_frames_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®é€£ç¶šç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """

    audio_receiver_size = st.sidebar.slider(
        "audio_receiver_size(å‡¦ç†éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ512):", 
        min_value=64, max_value=1024, value=512, step=64
    )
    energy_threshold = st.sidebar.slider(
        "energy_threshold(ç„¡éŸ³ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2000):", 
        min_value=100, max_value=5000, value=2000, step=100
    )
    amp_threshold = st.sidebar.slider(
        "amp_threshold(ç„¡éŸ³æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.3):", 
        min_value=0.00, max_value=1.00, value=0.30, step=0.05
    )
    # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤ 0.01 0.05 1.00ä»¥ä¸‹
        #amp_threshold = 0.30  #0.05
    silence_frames_threshold = st.sidebar.slider(
        "silence_frames_threshold(ãƒˆãƒªã‚¬ãƒ¼ç”¨é€£ç¶šç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100):", 
        min_value=20, max_value=300, value=60, step=20
    )
    #60ãŒBest,ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100
    timeout = st.sidebar.slider(
        "timeout(ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ç§’):", 
        min_value=1, max_value=3, value=1, step=1
    )
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    #st.session_state.audio_receiver_size = audio_receiver_size
    st.session_state.energy_threshold = energy_threshold
    st.session_state.amp_threshold = amp_threshold
    st.session_state.silence_frames_threshold = silence_frames_threshold
    st.session_state.timeout = timeout
    
    webrtc_ctx = webrtc_streamer(
        key="speech-to-text",
        desired_playing_state=True, 
        mode=WebRtcMode.SENDONLY,
        audio_receiver_size=audio_receiver_size, 
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": False, "audio": True},
    )

    sound_chunk = pydub.AudioSegment.empty()
    silence_frames = 0

    while True:
        if webrtc_ctx.audio_receiver:
            status_indicator.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")

            timeout=st.session_state.timeout
            energy_threshold=st.session_state.energy_threshold
            amp_threshold=st.session_state.amp_threshold
            silence_frames_threshold= st.session_state.silence_frames_threshold
            #print("audio_receiver_size =",audio_receiver_size)
            
            try:
                audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=timeout)
                #print("timeout=",timeout)
            except queue.Empty:
                status_indicator.write("No frame arrived.")
                sound_chunk = handle_queue_empty(sound_chunk, text_output)
                continue
            sound_chunk, silence_frames = process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold,amp_threshold)
            sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output)
        else:
            status_indicator.write("Stopping.")
            if len(sound_chunk) > 0:
                #print("len(sound_chunk)=",len(sound_chunk))
                text = transcribe(sound_chunk.raw_data)  #?
                text_output.write(text)
            break

def main():
    st.title("Real-time Speech-to-Text")
    
    
    status_indicator = st.empty()
    text_output = st.empty()
    app_sst(status_indicator,text_output)

if __name__ == "__main__":
    main()
