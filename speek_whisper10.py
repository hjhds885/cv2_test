import streamlit as st
import numpy as np
from streamlit_webrtc import WebRtcMode, webrtc_streamer
from pydub import AudioSegment
import queue, pydub, tempfile,  os, time
import whisper
import librosa
import torch.classes

#AudioSegment.converter = "/usr/bin/ffmpeg"
#pydub.AudioSegment.converter = "/usr/bin/ffmpeg"  #'c:\\FFmpeg\\bin\\ffmpeg.exe'

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    """
    Save an audio segment to a .wav file.
    Args:
        audio_segment (AudioSegment): The audio segment to be saved.
        base_filename (str): The base filename to use for the saved .wav file.
    """
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")



def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    Transcribe an audio segment using OpenAI's Whisper ASR system.
    Args:
        audio_segment (AudioSegment): The audio segment to transcribe.
        debug (bool): If True, save the audio segment for debugging purposes.
    Returns:
        str: The transcribed text.
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    ########################################################
    #ç„¡éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã€ãã‚Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
    threshold=65 #-50.0
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
    samples = np.array(audio_segment.get_array_of_samples())
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
    energy = np.sqrt(np.mean(samples**2))
    print("energy=",energy)     #55,65ã§ä»¥ä¸‹ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèª
    if energy < threshold  :
        print("ç„¡éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã€ãã‚Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹") 
    #######################################################
   

    #########################################################

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        file_size = os.path.getsize(tmpfile.name) 
        print("file_size=",file_size)
        if file_size < 390000:
            print("ç„¡éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚") 
            return "ç„¡éŸ³ã®ãŸã‚ã€éŸ³å£°ã¯èªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        #is_silent = is_silent_chunk(audio_segment) 
        #if is_silent:
            #print("ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¯ç„¡éŸ³ã§ã™:", is_silent)

       ###########################################################     
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        audio_path = tmpfile.name  #"audio.wav"
        y, sr = librosa.load(audio_path, sr=None)
        # ç„¡éŸ³ã¨ã¿ãªã™ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é–¾å€¤ï¼ˆé©å®œèª¿æ•´ï¼‰1e-3
        silence_threshold = 1e-4  # å°ã•ã„å€¤ã»ã©å³å¯†ã«ç„¡éŸ³ã‚’æ¤œå‡º
        # çŸ­æ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
        frame_length = 2048  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º
        hop_length = 512  # ã‚¹ãƒ©ã‚¤ãƒ‰å¹…
        energy = np.array([
            sum(abs(y[i:i + frame_length]**2))
            for i in range(0, len(y), hop_length)
        ])
        #print(f"çŸ­æ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼: {energy}") 
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒé–¾å€¤ä»¥ä¸‹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç„¡éŸ³ã¨åˆ¤æ–­
        is_silent = energy < silence_threshold
        # ãƒ‡ãƒãƒƒã‚°ç”¨: ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰²åˆã‚’ç¢ºèª
        silent_ratio = np.sum(is_silent) / len(is_silent)
        print(f"ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰²åˆ: {silent_ratio:.2%}") 
        if file_size < 390000:
            print("ç„¡éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚") 
            #return "ç„¡éŸ³ã®ãŸã‚ã€éŸ³å£°ã¯èªè­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ç„¡éŸ³ã‹ã©ã†ã‹ã®çµæœã‚’å‡ºåŠ›
        elif np.all(is_silent):
            print("ç„¡éŸ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚")
            
        else:
            print("éŸ³å£°ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
            
            # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
            #base:74M,small:244M,medium,large
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
            audio = whisper.load_audio(tmpfile.name)
            audio = whisper.pad_or_trim(audio)

            # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤ 0.01 0.05 1.00ä»¥ä¸‹
            silence_threshold1 = 0.30  #0.05
            print("ç„¡éŸ³ãƒ¬ãƒ™ãƒ«ï¼",np.max(np.abs(audio)))
            # ç„¡éŸ³ã®æ¤œå‡ºï¼šéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æŒ¯å¹…ãŒé–¾å€¤ã‚’ä¸‹å›ã‚‹å ´åˆã¯ç„¡éŸ³ã¨åˆ¤æ–­
            #éŸ³å£°ä¿¡å·ã®æœ€å¤§æŒ¯å¹…ãŒéå¸¸ã«ä½ã„å ´åˆã‚’ã€Œç„¡éŸ³ã€ã¨è¦‹ãªã—ã¦ã„ã¾ã™ã€‚
            if np.max(np.abs(audio)) < silence_threshold1:
                print("ç„¡éŸ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã•ãšçµ‚äº†ã—ã¾ã™ã€‚")
            else:
                # ç„¡éŸ³ã§ãªã„å ´åˆã®ã¿Whisperã§ã®æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
                # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
                answer = result['text']
                #print(answer)
                
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
                if result["text"].strip() == "":
                    print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹")
                else:
                    answer = result['text']
                    print(answer)
                    return answer
    tmpfile.close()  
    os.remove(tmpfile.name)
    
        ###############################################################
         
def frame_energy(frame):
    """
    Compute the energy of an audio frame.
    Args:
        frame (VideoTransformerBase.Frame): The audio frame to compute the energy of.
    Returns:
        float: The energy of the frame.
    """
    samples = np.frombuffer(frame.to_ndarray().tobytes(), dtype=np.int16)

    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚µãƒ³ãƒ—ãƒ«ã®ä¸€éƒ¨ã‚’å‡ºåŠ› 
    #print("Samples:", samples[:10])
    # NaNã‚„ç„¡é™å¤§ã®å€¤ã‚’é™¤å» 
    if not np.isfinite(samples).all(): 
        samples = samples[np.isfinite(samples)]
        #print("Filtered Samples:", samples[:10]) # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡ºåŠ›

    if len(samples) == 0: 
        return 0.0

    energy = np.sqrt(np.mean(samples**2)) 
    #print("Energy:", energy) 
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å‡ºåŠ› 
    return energy
 

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã™ã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚

    """
    for audio_frame in audio_frames:
        sound_chunk = add_frame_to_chunk(audio_frame, sound_chunk)

        energy = frame_energy(audio_frame)
        if energy < energy_threshold:
            silence_frames += 1 #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
        else:
            silence_frames = 0 #ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚

    return sound_chunk, silence_frames

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
   
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            #if is_silent_chunk(sound_chunk):
                #print("ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯å†…ã®ã™ã¹ã¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã™")
            #else: 
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³")
            frame_length_ms = 20 
            num_frames = len(sound_chunk) // frame_length_ms 
            #print("å…¨ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼",num_frames)
            #print("ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼",silence_frames)

            if num_frames-silence_frames > 100:
                text = transcribe(sound_chunk)
                text_output.write(text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)
            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0

       #else:
            #print("ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒå°‘ãªã„handle_silenceãƒ«ãƒ¼ãƒãƒ³")
            #text = transcribe(sound_chunk)
            #text_output.write(text)

    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    Handle the case where the audio frame queue is empty.
    Args:
        sound_chunk (AudioSegment): The current sound chunk.
        text_output (st.empty): The Streamlit text output object.
    Returns:
        AudioSegment: The updated sound chunk.
    """
    if len(sound_chunk) > 0:
        print("handle_queue_emptyãƒ«ãƒ¼ãƒãƒ³")   
        text = transcribe(sound_chunk)
        text_output.write(text)
        sound_chunk = pydub.AudioSegment.empty()

    return sound_chunk

def app_sst(
        status_indicator,
        text_output,
        timeout=3, 
        energy_threshold=2000,  #2000
        silence_frames_threshold=100,   #500  #100  #1024ã§Goodï¼ï¼
        ):
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã®ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã€‚ 
    ã“ã®é–¢æ•°ã¯WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆã—ã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã‚’é–‹å§‹ã—ã€
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã€ä¸€å®šã®ã—ãã„å€¤ã‚’è¶…ãˆã‚‹ç„¡éŸ³ãŒç¶šã„ãŸå ´åˆã«éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚
    å¼•æ•°ï¼š
        status_indicator: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå®Ÿè¡Œä¸­ã¾ãŸã¯åœæ­¢ä¸­ï¼‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        text_output: èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®Streamlitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        timeout (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¬ã‚·ãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ç§’ã€‚
        energy_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³ã¨è¦‹ãªã•ã‚Œã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000ã€‚
        silence_frames_threshold (int, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®é€£ç¶šç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    webrtc_ctx = webrtc_streamer(
        key="speech-to-text",
        mode=WebRtcMode.SENDONLY,
        audio_receiver_size=1024,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": False, "audio": True},
    )

    sound_chunk = pydub.AudioSegment.empty()
    silence_frames = 0

    while True:
        if webrtc_ctx.audio_receiver:
            status_indicator.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")

            try:
                audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=timeout)
            except queue.Empty:
                status_indicator.write("No frame arrived.")
                sound_chunk = handle_queue_empty(sound_chunk, text_output)
                continue
            sound_chunk, silence_frames = process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold)
            sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, silence_frames_threshold, text_output)
        else:
            status_indicator.write("Stopping.")
            if len(sound_chunk) > 0:
                #print("len(sound_chunk)=",len(sound_chunk))
                text = transcribe(sound_chunk.raw_data)  #?
                text_output.write(text)
            break

def main():
    st.title("Real-time Speech-to-Text")
    status_indicator = st.empty()
    text_output = st.empty()
    app_sst(status_indicator,text_output)

if __name__ == "__main__":
    main()
    #ã“ã®ä¿®æ­£ã§ã¯ã€asyncio.run(main())ã‚’ä½¿ç”¨ã—ã¦main()é–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’æ­£ã—ãç®¡ç†ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€NoneTypeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾ã™ã‚‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒæ¸›å°‘ã—ã¾ã™ã€‚
    #asyncio.run(main())
